---
title: "IEEE 754"
---

## Announcements

- **Welcome** to Systems in Rust
- **Action Items**:
  - "SHA-512" is due on next Friday
  - Lecture on numbers, inspired by the lab, while we wait.

## Today

- Understand the basics of floats
- Understand the language of floats
- Get a feel for when further investigation is required
- Have a few tools ready to help
- Be prepared to keep learning about IEEE floats

## Citation

- Yoinked
- [url](https://github.com/DigitalInBlue/CPPCon2015)
- Author John Farrier, Booz Allen Hamilton

## Mission Statement

- I'm a normal person.
    -   I think `5/2` is `2.5` or perhaps 2½.
- I'm a scientist.
    - I'm solving "Black-Scholes"
    - [Does anyone know what this is doing?](https://numba.pydata.org/numba-examples/examples/finance/blackscholes/results.html)
$$
d_2 = d_1 - \sigma\sqrt{T - t} = \frac{1}{\sigma\sqrt{T - t}}\left[\ln\left(\frac{S_t}{K}\right) + \left(r - q - \frac{1}{2}\sigma^2\right)(T - t)\right]
$$

## Common Fallacies

- "Floating point numbers are numbers"
    - `max(count())` Pinocchios
- "It’s floating point error"
    - "All floating point involves magical rounding errors"
- "Linux and Windows handle floats differently"
- "Floating point represents an interval value near the actual value"

## Common Fallacies 2.0

- "A double (an `f64`) holds 15 decimal places and I only need 3, so I have nothing to worry about"[^1]
- "My programming language does better math than your programming language"[^2]
- "Why can't computers just store whatever number I use"[^3]

[^1]: Grammatically, this statement should be "so I have nothing about which to worry."
[^2]: This is true in the special case that your programming language is Haskell.
[^3]: This is due to how many numbers there are.

## Anatomy of IEEE Floats

![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/IEEE754.svg/550px-IEEE754.svg.png)

## IEEE Float Specification

- IEEE 754-1985, IEEE 854-1987, IEEE 754-2008
    - These are paywalled.
    - Those are years.
- Provide for portable, provably consistent math
    - Consistent, not correct.

## Assurances

- Ensure some significant mathematical identities hold true:
    - $x + y  = y + x$
        - Symmetry of addition
    - $x + 0 = x$
        - Additive identity
    - $x = y \implies x - y = 0$
        - Identity under subtraction
        
## Assurances 2.0

$$
\frac{x}{\sqrt{x^2+y^2}} \leq 1
$$

- What is missing?

## IEEE Float Specification

- Ensure every floating point number is unique
- Ensure every floating point number has an opposite
    - _Zero_ is a special case because of this
- Specifies algorithms for addition, subtraction, multiplication, division, and square-root
    - Not really operations/relations! They are algorithms.
    
    
## Aside: Scientific Notation

> [Scientific notation is a way of expressing numbers that are too large or too small to be conveniently written in decimal form, since to do so would require writing out an inconveniently long string of digits.](https://en.wikipedia.org/wiki/Scientific_notation)

- In scientific notation, nonzero numbers are written in the form 

$$a \times 10^b$$

## Aside: Explanation


- In scientific notation, nonzero numbers are written in the form 

$$a \times 10^b$$


* $a$ (the coefficient or mantissa) is a number greater than or equal to 1 and less than 10 ($1 \le |a| < 10$).
* $10$ is the base.
* $b$ (the exponent) is an integer.


## Aside: Physical Examples

1.  **Speed of light:** The speed of light in a vacuum is approximately $300,000,000 \text{ m/s}$
$$
3 \times 10^8 \text{ m/s}
$$

2.  **Mass of an electron:** The mass of an electron is approximately $0.00000000000000000000000000091093837 \text{ g}$.
$$
9.1093837 \times 10^{-28} \text{ g}
$$

## Aside: Economic Examples

- We can use social science numbers.
- [Labor Market Outcomes of College Graduates by Major](https://www.newyorkfed.org/research/college-labor-market#--:explore:outcomes-by-major)
- Computer Science majors in 2023 have a \$80,000 median wage "early career"
	- $8.0000 \times 10^4$
- And 6.1% unemployment
  	- $6.1 \times 10^{-2}$

## IEEE Layout

- An approximation using scientific notation
    - $x = -1^s \times 2^e \times 1.m$
    - $x = -1^{\text{sign bit}} \times \text{base} 2^{\text{exponent}} \times 1.\text{mantissa}$
        - Where the mantissa is the technical term for the the digits after the decimal point.
        
## With Binary{.smaller}

- $x = -0b1^{\text{0b0}} \times 0b10^{\text{0b10000000}}  \times 0b1.10010010000111111011011$
- Express in memory as the concatenation:
    - `0b0` to `0b10000000` to `0b10010010000111111011011`
    - `01000000010010010000111111011011`
        
## Singles and Doubles
        
- 32 - bits = 1 sign bit + 8 exponent bits + 23 mantissa bits
    - `0b0` to `0b10000000` to `0b10010010000111111011011`
- 64 - bits = 1 sign bit + 12 exponent bits + 52 mantissa bits

## Understanding check

- What is the probability a *real number* $a \in \mathbb{R}$ has an exact float representation?
- What is the probability an *integer* $n \in \mathbb{Z}$ has an exact float representation?
- What is the probability a *course number* $n < 600$ has an exact float representation?


## Special Floats - NaN

- Divide by Zero
    - 1 / 0
- Not a Number (NaN)

```py
>>> from numpy import float32 as f32
>>> f32(1)/f32(0)
<stdin>:1: RuntimeWarning: divide by zero encountered in scalar divide
np.float32(inf)
>>> 1/0
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ZeroDivisionError: division by zero
```

## Special Floats - inf

- Signed Infinity
    - Overflow protection
```py
>>> f32(10) ** f32(100)
<stdin>:1: RuntimeWarning: overflow encountered in scalar power
np.float32(inf)
>>> 10.0 ** 10000
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
OverflowError: (34, 'Numerical result out of range')
```

## Infinity is signed 

- We can get negative infinity through a variety of means.

```py
>>> -(f32(10) ** f32(100))
np.float32(-inf)
```

- We note that this suppresses the overflow error use `()`
- In some languages this is not ever regarded as an overflow error, like Julia (where we have to use larger powers due to the f64 default)
```jl
julia> 10.0^10000
Inf

julia> -(10.0^10000)
-Inf

```

## Signed Zero

- Signed Zero
    - Underflow protection, preserves sign
    - + 0 =− 0
```py
>>> -1 / 10 ** 1000
-0.0
>>> 0.0 == (-1 / 10 ** 1000)
True
>>> -0.0
-0.0
>>> 0.0 - 0.0
0.0
```
- Works in base Python (e.g. don't need NumPy).
- Couldn't get it to work in Julia actually (went to -inf or NaN)



## Simple Example

- Floating point is great because it will work exactly how you expect.

```py
>>> 0.1 + 0.2 == 0.3
False
```

- More or less.

```py
>>> 0.1 + 0.2
0.30000000000000004
```

## Simple Example


- We shouldn't be suprised by this!
```py
>>> _ = [print(f"{f32(i/10):1.100}") for i in range(1,4)]
0.100000001490116119384765625
0.20000000298023223876953125
0.300000011920928955078125
>>> _ = [print(f"{f64(i/10):1.100}") for i in range(1,4)]
0.1000000000000000055511151231257827021181583404541015625
0.200000000000000011102230246251565404236316680908203125
0.299999999999999988897769753748434595763683319091796875
```

## Storage of 1.


- $x = -1^s \times 2^e \times 1.m$
- $x = -1^{\text{sign bit}} \times \text{base} 2^{\text{exponent}} \times 1.\text{mantissa}$
- $x = -1^{\text{sign bit}} \times \text{base} 2^{\text{exponent}} \times 1 + \text{mantissa} \times \frac{1}{2^n}$
- $1 = -1^0 \times 2^0 \times (1 + 0 \times \frac{1}{2^n}$
- So each of the sign bit, the base, and the mantissa are zero.
    - I should note I was unable to reproduce this in Rust or C.
    - I'm not really a floating pointer.

## Except...

- This is actually a little bit fake.
```rs
$ cat src/main.rs
fn main() {
    let x:f32 = 1.0;
    println!("{:032b}", x.to_bits());
}
$ cargo run
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.00s
     Running `target/debug/bits`
00111111100000000000000000000000
```
- What is that?

## Shift 127

> [While the exponent can be positive or negative, in binary formats it is stored as an unsigned number that has a fixed "bias" added to it.](https://en.wikipedia.org/wiki/Floating-point_arithmetic#Internal_representation)

- The IEEE 754 binary format specifies a hardware implementation of such that the exponent, be it positive or negative, in binary formats is stored as an unsigned number that has a fixed "bias" added to it.
- In the case of 32 bit floats, the bias is 127.

## Breakdown

- Take the following:
```sh
00111111100000000000000000000000
```
- That is:
    - Sign bit of `0`
    - Exponent of `01111111`
        - All zeroes and all ones are reserved for `NaN`, `inf`, etc.
    - Mantissa of 23 zeroes.
    
## How precise?

- I have referred to `f32` as having "32 bits of precision".
- How much precision is that?
- Well, let's set the least significant bit of the mantissa to one.
```{.rs filename="main.rs"}
fn main() {
    let x:f32 = 1.0;
    let mut b:u32 = x.to_bits();
    b |= 1;
    println!("{:032b}", b);
    println!("{:1.32}", f32::from_bits(b));
}
```

## "Epsilon"

- What do we get?
```sh
$ cargo run
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.00s
     Running `target/debug/bits`
00111111100000000000000000000001
1.00000011920928955078125000000000
```

- We refer to this value (less one) as [machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon), or perhaps $\varepsilon$.
- The difference between 1.0 and the next available floating point number.
- Much lower, of course, with double and quad precision.


## Easier: "Sig figs"

| Format | Sign | Exp.  | Mant. | Bits  | Bias | Prec. | Significance |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Half  | 1 | 5 | 10 | 16 | 15 | 11 | **3-4** |
| Single | 1 | 8 | 23 | 32 | 127 | 24 | **6-9** |
| Double | 1 | 11 | 52 | 64 | 1023 | 53 | **15-17** |
| Quad | 1 | 15 | 112 | 128 | 16383 | 113 | **33-36** |

- [Prof. Kahan, UCB EECS](https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF)

## Sig figs

- IEEE 754 was intended for *scientific computing*
- That is, not systems computing, software etc.
- Useful to us to think about information and how bits work.

> [Significant figures, also referred to as significant digits, are specific digits within a number that is written in positional notation that carry both reliability and necessity in conveying a particular quantity.](https://en.wikipedia.org/wiki/Significant_figures)

## Small Values

- For 32-bit floats, the minimum base 10 exponent is -36.
- How is $1.0 \times 10^{-37}$ represented?

```rs
// In Rust, we may use '0b' prefix for binary representation.
// In Rust, we may us underscores within numerical values.
0b0_00000000_11011001110001111101110
```

## Test it

- Write a simple test...
```{.rs filename="src/main.rs"}
fn main() {
    let b:u32 = 0b0_00000000_11011001110001111101110;
    println!("{:032b}", b);
    println!("{:1.100}", f32::from_bits(b));
    let x:f32 = 10.0_f32.powf(-38.0_f32);
    println!("{:032b}", x.to_bits());
    println!("{:1.100}", x);
}
```
- Results are as expected.
```sh
00000000011011001110001111101110
0.0000000000000000000000000000000000000099999993504564039245746141539976645128551939195729831580121175
00000000011011001110001111101110
0.0000000000000000000000000000000000000099999993504564039245746141539976645128551939195729831580121175
```


## "Denormalized Number"

- Numbers that have a zero exponent
- Required when the exponent is below the minimum exponent
- Helps prevent underflow
- Generally speaking, if you are using one these, the math is about to get wronger than you'd think.
- The good (???) language C++ provides a `std::nextafter` for which Rust crates exist.

## Floating Point Precision


- Representation is not uniform between numbers
- Most precision lies between 0.0 and 0.1
- Precision falls away

## Visually

![](https://dwayneneed.github.io/static/img/2010-05-06-fun-with-floating-point_6bit-float-distribution.png)

- [Fun with floating point, Dwayne Need](https://dwayneneed.github.io/.net/2010/05/06/fun-with-floating-point.html)

## Floating Point Precision

The number of floats from 0.0

- ... to 0.1 = 1,036,831,949
- ... to 0.2 = 8,388,608
- ... to 0.4 = 8,388,608
- ... to 0.8 = 8,388,608
- ... to 1.6 = 8,388,608
- ... to 3.2 = 8,388,608


## Errors in Floating Point

- Storage of $\pi$
```py
>>> from numpy import pi as pi
>>> # copy paste Wolfram|Alpha
>>> '3.14159265358979323846264338327950288419716939937510582097494459230781640628'
'3.14159265358979323846264338327950288419716939937510582097494459230781640628'
>>> f'{pi:1.100}'
'3.141592653589793115997963468544185161590576171875'
```
- They differ fairly early on!

## Measuring Error: "Ulps"

- Units in Last Place

> [In computer science and numerical analysis, unit in the last place or unit of least precision (ulp) is the spacing between two consecutive floating-point numbers, i.e., the value the least significant digit (rightmost digit) represents if it is 1. It is used as a measure of accuracy in numeric calculations.](https://en.wikipedia.org/wiki/Unit_in_the_last_place)

## Generators

- By convention we check number theoretic results with lazily evaluated generator expressions in Python.
- The following generators all powers of two for a which adding one does not change the value within Python floating points.
```py
from itertools import count

gen = (i for i in count() if 2.0 ** float(i) + 1.0 == 2.0 ** float(i))

x = next(gen)

print('2.0 ** x + 0.0 = ', 2.0 ** x + 0.0)
print('2.0 ** x + 1.0 = ', 2.0 ** x + 1.0)
```

## Iterators

- Rust doesn't require iterators, supporting the infinite iterator by default using `unwrap`.
```rs
let x = (0..).find(|&i| f64::powi(2.0, i) + 1.0 == f64::powi(2.0, i)).unwrap();

let base = f64::powi(2.0, x);

// Prints the result (2^53 = 9007199254740992.0)
println!("2.0 ** x + 0.0 = {:.1}", base);
println!("2.0 ** x + 1.0 = {:.1}", base + 1.0);
```
- I haven't used `find` before, this is an LLM donation to our knowledge base.
- By the way, that is basically `2 ** 53` and there are 52 bits of mantissa precision.

## On "Ulps"

- We can measure error using ULPs.

> [The gap between the two floating-point numbers nearest to x, even if x is one of )them.](https://www.cs.berkeley.edu/~wkahan/LOG10HAF.TXT)

```py
>>> from numpy import float32 as f32, pi as pi, float64 as f64
>>> f64(pi) - f64(f32(pi))
np.float64(-8.742278012618954e-08)
```

## Rounding

- We have some guarantees:
    - IEEE 754 requires that that elementary arithmetic operations are correctly rounded to within 0.5 ulps
    - Transcendental functions are generally rounded to between 0.5 and 1.0 ulps
        - Transcendental as in $\pi$ or $e$

- ~9 ulps

# Relative Error

- The difference between the "real" number and the approximated number, divided by the "real" number.

```py
>>> f64(f32(pi))/f64(pi)
np.float64(1.0000000278275352)
```

- So for `pi` in `f32`, around $2.9 \times 10^{-8}$

## Rounding Error

- Induced by approximating an infinite range of numbers into a finite number of bits
- Rounding is:
    - Towards the nearest
    - Towards zero
    - Towards positive infinity (round up)
    - Towards negative infinity (round down)

## Rounding Error

- What about rounding the half-way case? (i.e. 0.5)
    - Round Up vs. Round Even
- Correct Rounding:
    - Basic operations (add, subtract, multiply, divide, sqrt) should return the number nearest the
       mathematical result.
    - If there is a tie, round to the number with an even mantissa

## Implementation

- "Guard Bit", "Round Bit", "Sticky Bit"
- Only used while doing calculations
    - Not stored in the float itself
    - The mantissa is shifted in calculations to align radix
- The guard bits and round bits are extra precision
- The sticky bit is an OR of anything that shifts through it

```rs
0_00000000_00000000000000000000000_G_R_S..
```

## Special bits

- "Guard Bit", "Round Bit", "Sticky Bit"
- **[G][R][S]**
- **[0][-][-]** - Round Down (do nothing)
- **[1][0][0]** - Round Up if the mantissa LSB is 1
- **[1][0][1]** - Round Up
- **[1][1][0]** - Round Up
- **[1][1][1]** - Round Up

```rs
0_00000000_00000000000000000000000_G_R_S..
```

## Significance Error

- Compute the Area of a Triangle
    - Heron’s Formula:
       - 𝐴 = 𝑥+𝑦 2 +𝑍 𝑥+𝑦 2 +𝑍 −𝑥 𝑥+𝑦 2 +𝑍 −𝑦 𝑥+𝑦 2 +𝑍 −𝑧
    - Kahan’sAlgorithm:
       - Sort x, y, z such that 𝑥≥𝑦 ≥𝑧
       - If 𝑧< 𝑥−𝑦, then no such triangle exists.
       - Else 𝐴 = (𝑥+𝑦+𝑧) × 𝑧−(𝑥−𝑦 )× 4 (𝑧+𝑥−𝑦 )×(𝑥+𝑦−𝑧)


Significance Error

```
/// Area of a triangle
/// From http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html
/// From https://www.cs.berkeley.edu/~wkahan/Triangle.pdf
TEST(CPPCon2015, AreaOfATriangleFloat)
{
const auto a = 100000.0f;
const auto b = 99999.99979f;
const auto c = 0.00029f;
```
```
ASSERT_TRUE(a >= b);
ASSERT_TRUE(b >= c);
```
```
auto heronsFormula = HeronsFormula(a, b, c);
auto kahansFormula = KahansFormula(a, b, c);
EXPECT_NE(kahansFormula, heronsFormula);
cppCon() << "Kahan: " << kahansFormula;
cppCon() << "Heron: " << heronsFormula;
cppCon() << "delta: " << kahansFormula - heronsFormula << "\n";
}
```

Significance Error

Heron: 0.000000000000000000000000000000

Kahan: 14.500000000000000000000000000000

delta: 14.500000000000000000000000000000


Significance Error

Heron: 0.000000000000000000000000000000

Kahan: 14.500000000000000000000000000000

delta: 14.500000000000000000000000000000

Hint: The Answer is 10.0


Significance Error

Heron: 0.000000000000000000000000000000

Kahan: 14.500000000000000000000000000000

delta: 14.500000000000000000000000000000

Heron: 9.999999809638328700000000000000

Kahan: 10.000000077021038000000000000000

delta: 0.000000267382709751018410000000


Significance Error - Use Stable Algorithms

- Loss of Significance
    - Keep big numbers with big numbers, little numbers with little numbers.
- Parentheses can help
- Analysis of Algorithms is Critical
- The compiler won’t re-arrange your math if it cannot prove it would yield the same

result, even if the computation would be faster

- 𝑥 = 𝑏− 𝑎+𝑏 −𝑎 should not be replaced with 𝑥= 0
- See the "Kahan’sAlgorithm" example


Significance Error – Simulation Time

- Time is used to compute distance, velocity, acceleration
    - High frequency phenomena
    - Sensors: Doppler shift, pulse compression, PRF
- These computed values feed into other computed values which may or may not

require a time component

- Thousands of computations per frame of simulation
    - Thousands of little compounding errors per frame
- Combine with poor or nonexistent testing


Significance Error – Simulation Time

- Accumulate Time (works for arbitrary

time steps)

auto totalFrames = size_t(0);
auto frameLength = 0.01f;
auto simTime = 0.0f;

// Run 120 Frames
auto simTime120Frames = 1.20f;
for(; totalFrames < 120; totalFrames++)
{
simTime += frameLength;
}

```
auto totalFrames = size_t(0);
auto frameLength = 0.01f;
auto simTime = 0.0f;
```
```
// Run 120 Frames
auto simTime120Frames = 1.20f;
totalFrames = 120;
simTime = totalFrames * frameLength;
```
- Delta Time (works for fixed time steps)


Significance Error – Simulation Time

Accumulate Time (works for arbitrary time steps)

- 1.199999999999999955591079014994 !=
    **1.199999213218688964843750000000**
    (0.000000786781310990747329014994)
- 12.000000000000000000000000000000 !=
    **12.000179290771484375000000000000** (-
    0.000179290771484375000000000000)
- 120.000000000000000000000000000000 !=
    **120.007225036621093750000000000000** (-
    0.007225036621093750000000000000)
- 600.000000000000000000000000000000 !=
    **600.274414062500000000000000000000** (-
    0.274414062500000000000000000000)
- 3600.000000000000000000000000000000 !=
    **3603.204101562500000000000000000000** (-
    3.204101562500000000000000000000)

Delta Time (works for fixed time steps)

- 1.200000047683715820312500000000 !=
    **1.199999928474426269531250000000**
    (0.000000119209289550781250000000)
- 12.000000000000000000000000000000 ==
    **12.000000000000000000000000000000**
    (0.000000000000000000000000000000)
- 120.000000000000000000000000000000 ==
    **120.000000000000000000000000000000**
    (0.000000000000000000000000000000)
- 600.000000000000000000000000000000 ==
    **600.000000000000000000000000000000**
    (0.000000000000000000000000000000)
- -3600.000000000000000000000000000000 ==
    **3600.000000000000000000000000000000**
    (0.000000000000000000000000000000)


Significance Error – Simulation Time

- "Sub-Microsecond Precision"
    - 1.2345e-6 seconds per frame (810044.5 Hz)


Significance Error – Simulation Time

- "Sub-Microsecond Precision"
    - 1.2345e-6 seconds per frame (810044.5 Hz)

### 0.012345000170171261000000000000 !=

### 0.0148140005767345430

### (-0.002469000406563282000000000000)

### 12.345000267028809000000000000000 !=

### 11.701118469238281000000000000000

### (0.643881797790527340000000000000)


Significance Error – Don’t use IEEE Floats?

- Use integers
    - Very fast
    - Trade more precision for less range
    - Only input/output may be impacted by floating point conversions
    - Financial applications represent dollars as only cents or tenth’s of cents
- Use a math library
    - Slower
    - Define your own level of accuracy
    - MPFR (w/C++ Wrapper), TTMath, Boost, GMP C++
    - CRlibm(Correctly Rounded Mathematical Library)

(Store simTimeas uint64_tand get microsecond precision for 584555 years.)


Algebraic Assumption Error

- Mathematical Identities
    - Traditional identities (associative, commutative, distributive) do not hold
- Distributive Rule does not apply: 𝑥 ×𝑦 −𝑥 × 𝑧 ≠ 𝑥 (𝑦 −𝑧)
- Associative Rule does not apply: 𝑥 + 𝑦 +𝑧 ≠ 𝑥 +𝑦 +𝑧
- Cannot interchange division and multiplication:

### 𝑥

### 10. 0

≠ 𝑥 × 0. 1

Does a naïve compiler make these assumptions too?

https://msdn.microsoft.com/library/aa289157.aspx


Algebraic Assumption Error

```
const auto oneRadian = 0.15915494309f;
const auto control = 0.000000000015915494309f;
```
```
const auto oneRadianMultiplied = oneRadian * 1.0e-10f;
const auto oneRadianDivided = oneRadian / 1.0e10f;
```

Algebraic Assumption Error

```
const auto oneRadian = 0.15915494309f;
const auto control = 0.000000000015915494309f;
```
```
const auto oneRadianMultiplied = oneRadian * 1.0e-10f;
const auto oneRadianDivided = oneRadian / 1.0e10f;
```
Control: 0.000000000015915494616658421000
x*1.0e-10f: 0.000000000015915494616658421000(0.000000000000000000000000000000)
x/1.0e10f: 0.00000000001591549 2881934945000 (0.000000000000000001734723475977)
Relative Error: 0.000000108995891423546710000000


Floating Point Exceptions

- Enable floating point exceptions to be alerted when things go awry.

```
𝑥is the exact result of the operation
α= 192 for single precision, 1536 for double
𝑥𝑚𝑎𝑥= 1. 111 ... 111 × 2 𝑒𝑚𝑖𝑛.
See <cfenv> -Floating Point Environment
```
IEEE 754 Exception Result when traps disabled Argument to trap handler

overflow ±∞ or ±𝑥𝑚𝑎𝑥 round(𝑥 2 −𝛼)

underflow 0, 2 𝑒𝑚𝑖𝑛or denormalized round(𝑥 2 𝛼)

divide by zero ±∞ invalid operation

invalid NaN invalid operation

inexact round( _x_ ) round( _x_ )


But Wait! There’s More!

- Binary to Decimal Conversion Error
- Summation Error
- Propagation Error
- Underflow, Overflow
- Type Narrowing/Widening Rules


Miscellaneous Notes

[http://preshing.com/images/float-point-perf.png](http://preshing.com/images/float-point-perf.png)
https://github.com/DigitalInBlue/CPPCon2015


Use Your Compiler’s Output

- warning C4244: ‘initializing’ : conversion from ‘double’ to

‘float’, possible loss of data

- warning C4056: overflow in floating point constant arithmetic
- warning C4305: 'identifier' : truncation from 'type1' to

'type2'

- warning: conversion to 'float' from 'int' may alter its value
- warning: floating constant exceeds range of ‘double’


Fused Multiply-Add (FMA)

- a = a + (b * c); a += b * c;
- Multiplier-accumulator (MAC Unit)
- One rounding
- Compiler Options
    - GCC = -mfma
    - VC++ = #pragma fp_contract (off)
- Reference: FMA3, FMA4
    - [http://en.wikipedia.org/wiki/FMA_instruction_set](http://en.wikipedia.org/wiki/FMA_instruction_set)


Streaming SIMD Extensions (SSE)

- SSE can provide significant performance gains
    - Supports integer, floating point, logical, conversion, shift, and shuffle operations
- C, C++, Fortran do not natively support SSE, but compiler-specific support exists


Float Tricks

```
/// Fast reciprocal square root approximation for x > 0.25
/// Quake’s float Q_rsqrt(float number) is much more entertaining.
inline float FastInvSqrt(float x)
{
int tmp = ((0x3f800000 << 1) + 0x3f800000 - *(long*)&x) >> 1;
auto y = *(float*)&tmp;
return y * (1.47f – 0.47f * x * y * y);
}
```

Testing

- Design for Numerical Stability
- Perform Meaningful Testing
- Document assumptions
- Track sources of approximation
- Quantify goodness
    - Well conditioned algorithms
- Backward error analysis
    - Are the outputs identical for slightly
       modified inputs?

```
TEST(CPPCon2015, pointOnePlusPointTwo)
{
auto zeroPointOne = 0.1f;
auto zeroPointTwo = 0.2f;
auto zeroPointThree = 0.3f;
auto sum = zeroPointOne + zeroPointTwo;
```
```
EXPECT_DOUBLE_EQ(0.3f, zeroPointThree);
EXPECT_EQ(0.3f, zeroPointThree);
EXPECT_EQ(0.3f, sum);
```
```
EXPECT_EQ(zeroPointThree, sum);
EXPECT_DOUBLE_EQ(zeroPointThree, sum);
}
```

[http://xkcd.com/217/](http://xkcd.com/217/)
[http://www.explainxkcd.com/wiki/index.php/217:_e_to_the_pi_Minus_pi](http://www.explainxkcd.com/wiki/index.php/217:_e_to_the_pi_Minus_pi)

```
https://github.com/DigitalInBlue/CPPCon2015
```

# Demystifying Floating Point

John Farrier, Booz Allen Hamilton

```
https://github.com/DigitalInBlue/CPPCon2015
```

